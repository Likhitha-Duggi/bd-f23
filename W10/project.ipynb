{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70de5895-00e1-4aa2-97d3-827ee91bf435",
   "metadata": {},
   "source": [
    "# BIG DATA PROJECT - HADOOP HEROES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c84e6d5-1da8-43e3-a344-3ca6a8c9440a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35fa0a2f-cf73-47d3-9fb2-05c8cdca56e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/05 17:20:25 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 10.21.10.196 instead (on interface eth0)\n",
      "23/11/05 17:20:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/05 17:20:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session WebUI Port: 4040\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .appName(\"ISM6562 Spark Project\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Let's get the SparkContext object. It's the entry point to the Spark API. It's created when you create a sparksession\n",
    "sc = spark.sparkContext  \n",
    "sc.setLogLevel(\"ERROR\") # only display errors (not warnings)\n",
    "\n",
    "# note: If you have multiple spark sessions running (like from a previous notebook you've run), \n",
    "# this spark session webUI will be on a different port than the default (4040). One way to \n",
    "# identify this part is with the following line. If there was only one spark session running, \n",
    "# this will be 4040. If it's higher, it means there are still other spark sesssions still running.\n",
    "spark_session_port = spark.sparkContext.uiWebUrl.split(\":\")[-1]\n",
    "print(\"Spark Session WebUI Port: \" + spark_session_port)\n",
    "\n",
    "# It's best if you find that the port number displayed below is not 4040, then you should shut down all other spark sessions and \n",
    "# run this code again. If you don't, you may have trouble accessing the data in the spark-warehouse directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8f58cef-4d8c-4b74-8713-d63de7a24b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://linux:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ISM6562 Spark Project</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f4cbf587040>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0e41e11-fa59-4d53-9f7a-e2091bf85e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c4e1f87-3255-4a1b-83cf-264ae69e9d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "|   w10_db|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.sql(\"show databases\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a3b8ec2-c6bb-41e2-9b12-12d95562d381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tables = spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21889ead-2042-4028-ad39-fd9808dfa98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data to warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ac0be0c-7a49-46a7-a5da-86f73012d810",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+------------+------------+-----------+-----+----------+------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|PULocationID|DOLocationID|fare_amount|extra|tip_amount|total_amount|airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+------------+------------+-----------+-----+----------+------------+-----------+\n",
      "|       1|    02-01-2022 00:06|     02-01-2022 00:19|              1|          5.4|         138|         252|       17.0| 1.75|       3.9|       23.45|       1.25|\n",
      "|       1|    02-01-2022 00:38|     02-01-2022 00:55|              1|          6.4|         138|          41|       21.0| 1.75|       0.0|        30.1|       1.25|\n",
      "|       1|    02-01-2022 00:03|     02-01-2022 00:26|              1|         12.5|         138|         200|       35.5| 1.75|       0.0|        44.6|       1.25|\n",
      "|       2|    02-01-2022 00:08|     02-01-2022 00:28|              1|         9.88|         239|         200|       28.0|  0.5|       0.0|        34.8|        0.0|\n",
      "|       2|    02-01-2022 00:06|     02-01-2022 00:33|              1|        12.16|         138|         125|       35.5|  0.5|      8.11|       48.66|       1.25|\n",
      "+--------+--------------------+---------------------+---------------+-------------+------------+------------+-----------+-----+----------+------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trip = spark.read.csv('data/yellow_tripdata_2022-02.csv', header=True, inferSchema=True);\n",
    "\n",
    "# display the first 5 rows of the dataframe\n",
    "trip.show(5);"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f6e0b405-da7c-4099-83b7-39948682d305",
   "metadata": {},
   "source": [
    "trip.createOrReplaceTempView(\"trip_tmp_view\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aaafad85-2875-4247-a146-068a042ba6fa",
   "metadata": {},
   "source": [
    "df = spark.sql(\"SELECT * FROM trip_tmp_view\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a98dd2cb-4aba-4d68-93a9-b296e245d462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tables = spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a922f776-61e2-4ab7-874c-e2e6e3003162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f20392b7-0843-4b65-bf78-4bddee7952b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save table in spark data warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85741792-3de4-4145-90eb-f22efc92da1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS w10_db;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94e7d65c-6e3d-4f05-b567-a0b77994f076",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "trip.write.mode(\"overwrite\").saveAsTable(\"w10_db.trip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99387fbb-709d-4276-be6b-02cb23ddf23e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='boston', catalog='spark_catalog', namespace=['w10_db'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='trip', catalog='spark_catalog', namespace=['w10_db'], description=None, tableType='MANAGED', isTemporary=False)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables('w10_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ab828ad-44e4-4ef2-9f3c-314e952af18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+------------+------------+-----------+-----+----------+------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|PULocationID|DOLocationID|fare_amount|extra|tip_amount|total_amount|airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+------------+------------+-----------+-----+----------+------------+-----------+\n",
      "|       2|    02-01-2022 20:44|     02-01-2022 21:08|              1|         5.74|         107|           7|       20.5|  0.5|      4.86|       29.16|        0.0|\n",
      "|       1|    02-01-2022 20:35|     02-01-2022 20:44|              1|          1.3|         230|         229|        7.0|  3.0|       0.0|        10.8|        0.0|\n",
      "|       2|    02-01-2022 20:11|     02-01-2022 20:33|              1|         4.37|          79|         236|       18.0|  0.5|      4.36|       26.16|        0.0|\n",
      "|       2|    02-01-2022 20:49|     02-01-2022 20:52|              1|         0.46|         162|         229|        4.0|  0.5|       5.0|        12.8|        0.0|\n",
      "|       1|    02-01-2022 20:33|     02-01-2022 20:37|              1|          0.6|         211|         113|        4.5|  3.0|      1.65|        9.95|        0.0|\n",
      "|       1|    02-01-2022 20:48|     02-01-2022 21:06|              1|          2.3|         170|          48|       12.5|  3.0|      3.26|       19.56|        0.0|\n",
      "|       1|    02-01-2022 20:03|     02-01-2022 20:12|              1|          1.7|         236|         237|        8.5|  3.0|      3.05|       15.35|        0.0|\n",
      "|       1|    02-01-2022 20:49|     02-01-2022 20:58|              1|          1.8|         263|         237|        9.0|  3.0|      2.55|       15.35|        0.0|\n",
      "|       2|    02-01-2022 20:04|     02-01-2022 20:15|              2|         2.32|         161|         113|       10.0|  0.5|      2.48|       16.28|        0.0|\n",
      "|       2|    02-01-2022 20:35|     02-01-2022 20:48|              2|         3.42|         113|         237|       12.5|  0.5|       2.0|        18.3|        0.0|\n",
      "|       2|    02-01-2022 20:55|     02-01-2022 21:08|              2|         2.26|         161|         236|       11.0|  0.5|      2.22|       17.02|        0.0|\n",
      "|       2|    02-01-2022 20:58|     02-01-2022 21:28|              4|         5.72|         249|         181|       22.5|  0.5|      5.26|       31.56|        0.0|\n",
      "|       2|    02-01-2022 20:03|     02-01-2022 20:08|              1|          0.6|         236|         263|        5.0|  0.5|       2.2|        11.0|        0.0|\n",
      "|       2|    02-01-2022 20:18|     02-01-2022 20:22|              1|         0.83|         263|         141|        5.0|  0.5|       0.0|         8.8|        0.0|\n",
      "|       2|    02-01-2022 20:27|     02-01-2022 20:34|              1|          1.5|         236|         239|        7.0|  0.5|      2.16|       12.96|        0.0|\n",
      "|       2|    02-01-2022 20:39|     02-01-2022 20:46|              1|         1.42|         239|         237|        7.0|  0.5|       0.0|        10.8|        0.0|\n",
      "|       2|    02-01-2022 20:59|     02-01-2022 21:08|              2|         1.96|          43|         151|        9.0|  0.5|       3.2|        16.0|        0.0|\n",
      "|       2|    02-01-2022 20:15|     02-01-2022 20:23|              1|         2.11|         234|         229|        9.0|  0.5|      2.56|       15.36|        0.0|\n",
      "|       2|    02-01-2022 20:38|     02-01-2022 20:45|              1|         1.21|         161|         141|        6.5|  0.5|      2.06|       12.36|        0.0|\n",
      "|       2|    02-01-2022 19:58|     02-01-2022 20:04|              1|         1.06|         107|          90|        6.5|  0.5|       1.0|        11.3|        0.0|\n",
      "+--------+--------------------+---------------------+---------------+-------------+------------+------------+-----------+-----+----------+------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"SELECT * FROM w10_db.trip\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c09ea736-9104-4097-a4f2-41d75bc65da7",
   "metadata": {},
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c54b5a5b-740b-4489-83de-77a551772bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1043585"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efbd1b6e-e1d5-465f-8706-b1954e681303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[VendorID: int, tpep_pickup_datetime: string, tpep_dropoff_datetime: string, passenger_count: int, trip_distance: double, PULocationID: int, DOLocationID: int, fare_amount: double, extra: double, tip_amount: double, total_amount: double, airport_fee: double]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac81bcc5-b248-4df6-9543-99eee70b640e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1043585"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84444499-fbef-43c9-ab00-b545d33071c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: string (nullable = true)\n",
      " |-- tpep_dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dea49ca8-e16e-4698-9a8a-b51a0678e931",
   "metadata": {},
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "# Assuming your DataFrame is named 'df'\n",
    "df = df.withColumn(\"tpep_pickup_datetime\", to_timestamp(df[\"tpep_pickup_datetime\"], \"MM-dd-yyyy HH:mm\"))\n",
    "df = df.withColumn(\"tpep_dropoff_datetime\", to_timestamp(df[\"tpep_dropoff_datetime\"], \"MM-dd-yyyy HH:mm\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "be9d5a50-0931-4304-a702-c86d5234296b",
   "metadata": {},
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610a450e-6974-4ad4-acb4-3ec86a14f81d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b74341ba-46c9-4752-8d96-ebb26618d89b",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1548517e-19e9-41df-bce8-c4471d7991f6",
   "metadata": {},
   "source": [
    "from pyspark.sql.types import IntegerType,BooleanType,DateType, StringType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e0caa93-85b0-4bc0-b975-ef1b8820364a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data=df.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30c55d99-cc59-4b0a-8fd6-6c7b8eb12565",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "# Use StringIndexer to convert the categorical columns to hold numerical data\n",
    " \n",
    "tpep_pickup_datetime_indexer = StringIndexer(inputCol='tpep_pickup_datetime',outputCol='tpep_pickup_datetime_index',handleInvalid='keep')\n",
    "tpep_dropoff_datetime_indexer = StringIndexer(inputCol='tpep_dropoff_datetime',outputCol='tpep_dropoff_datetime_index',handleInvalid='keep')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40c2bccc-058b-4dc2-8a14-ac3bfd13452c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VendorID',\n",
       " 'tpep_pickup_datetime',\n",
       " 'tpep_dropoff_datetime',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'fare_amount',\n",
       " 'extra',\n",
       " 'tip_amount',\n",
       " 'total_amount',\n",
       " 'airport_fee']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c88769-ebc3-42a2-8d87-70ea1baa2575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b65b4ea-0d8d-4850-aef1-a2030c0ed05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "# Vector assembler is used to create a vector of input features\n",
    " \n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        'passenger_count',\n",
    "        'trip_distance',\n",
    "        'airport_fee',\n",
    "        'PULocationID',\n",
    "        'DOLocationID',\n",
    "        'tpep_dropoff_datetime_index',\n",
    "        'tpep_pickup_datetime_index'\n",
    "    ],\n",
    "    outputCol=\"features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac89eb16-1e0b-4619-9b42-2a7fcd086213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Pipeline is used to pass the data through indexer and assembler simultaneously. Also, it helps to pre-rocess the test data\n",
    "# in the same way as that of the train data\n",
    "# https://spark.apache.org/docs/latest/ml-pipeline.html\n",
    " \n",
    "pipe = Pipeline(stages=[\n",
    "    tpep_dropoff_datetime_indexer,\n",
    "    tpep_pickup_datetime_indexer,\n",
    "    assembler\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3cb15d7-225c-430b-9d65-9a931e72bd3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fitted_pipe=pipe.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be48a194-1248-4058-a9ef-4a1b5d347fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+------------+------------+-----------+-----+----------+------------+-----------+---------------------------+--------------------------+--------------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|PULocationID|DOLocationID|fare_amount|extra|tip_amount|total_amount|airport_fee|tpep_dropoff_datetime_index|tpep_pickup_datetime_index|            features|\n",
      "+--------+--------------------+---------------------+---------------+-------------+------------+------------+-----------+-----+----------+------------+-----------+---------------------------+--------------------------+--------------------+\n",
      "|       1|    02-01-2022 20:00|     02-01-2022 20:03|              1|          0.7|         142|          48|        5.0|  3.5|       0.0|         9.3|        0.0|                     4907.0|                    5681.0|[1.0,0.7,0.0,142....|\n",
      "|       1|    02-01-2022 20:00|     02-01-2022 20:06|              1|          1.1|         162|         186|        6.5|  3.0|      2.05|       12.35|        0.0|                     3876.0|                    5681.0|[1.0,1.1,0.0,162....|\n",
      "|       1|    02-01-2022 20:00|     02-01-2022 20:06|              1|          1.2|          68|         158|        6.0|  3.0|      1.95|       11.75|        0.0|                     3876.0|                    5681.0|[1.0,1.2,0.0,68.0...|\n",
      "|       1|    02-01-2022 20:00|     02-01-2022 20:07|              1|          1.1|         237|         236|        7.0|  3.0|      1.25|       12.05|        0.0|                     4136.0|                    5681.0|[1.0,1.1,0.0,237....|\n",
      "|       1|    02-01-2022 20:00|     02-01-2022 20:07|              1|          1.4|         163|         237|        7.0|  3.0|       0.0|        10.8|        0.0|                     4136.0|                    5681.0|[1.0,1.4,0.0,163....|\n",
      "+--------+--------------------+---------------------+---------------+-------------+------------+------------+-----------+-----+----------+------------+-----------+---------------------------+--------------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data=fitted_pipe.transform(train_data)\n",
    "train_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af9ce14b-d5f3-48c9-bd3e-be95a808e73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+------------+------------+-----------+-----+----------+------------+-----------+---------------------------+--------------------------+--------------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|PULocationID|DOLocationID|fare_amount|extra|tip_amount|total_amount|airport_fee|tpep_dropoff_datetime_index|tpep_pickup_datetime_index|            features|\n",
      "+--------+--------------------+---------------------+---------------+-------------+------------+------------+-----------+-----+----------+------------+-----------+---------------------------+--------------------------+--------------------+\n",
      "|       1|    02-01-2022 20:00|     02-01-2022 20:03|              1|          0.9|         236|          43|        5.0|  3.5|       0.0|         9.3|        0.0|                     4907.0|                    5681.0|[1.0,0.9,0.0,236....|\n",
      "|       1|    02-01-2022 20:00|     02-01-2022 20:04|              0|          0.9|         230|         186|        5.0|  3.0|       2.0|        10.8|        0.0|                     3875.0|                    5681.0|[0.0,0.9,0.0,230....|\n",
      "|       1|    02-01-2022 20:00|     02-01-2022 20:04|              2|          0.7|          43|         236|        4.5|  3.5|       0.0|         8.8|        0.0|                     3875.0|                    5681.0|[2.0,0.7,0.0,43.0...|\n",
      "|       1|    02-01-2022 20:00|     02-01-2022 20:05|              1|          0.9|         162|         137|        6.0|  3.0|      1.95|       11.75|        0.0|                     6236.0|                    5681.0|[1.0,0.9,0.0,162....|\n",
      "|       1|    02-01-2022 20:00|     02-01-2022 20:09|              1|          1.0|          90|         158|        7.0|  3.0|      2.15|       12.95|        0.0|                     4137.0|                    5681.0|[1.0,1.0,0.0,90.0...|\n",
      "+--------+--------------------+---------------------+---------------+-------------+------------+------------+-----------+-----+----------+------------+-----------+---------------------------+--------------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data=fitted_pipe.transform(test_data)\n",
    "test_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48d921e8-4d16-4a96-aa30-eb4cb80bf999",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# For those interested in utilizing the ML/AI power of Tensorflow with Spark....\n",
    "# https://github.com/tensorflow/ecosystem/tree/master/spark/spark-tensorflow-distributor\n",
    "\n",
    "# In this course, we'll use the SparkML (admitedely, it's not as powerful as Tensorflow, but \n",
    "# it's easy to use and demonstrate ML on a Spark Cluster)\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr_model = LinearRegression(labelCol='fare_amount')\n",
    "fit_model = lr_model.fit(train_data.select(['features','fare_amount']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5bc99b7a-4e23-442b-ab45-a8db0e16dc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+------------+------------+-----------+-----+----------+------------+-----------+---------------------------+--------------------------+--------------------+------------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|PULocationID|DOLocationID|fare_amount|extra|tip_amount|total_amount|airport_fee|tpep_dropoff_datetime_index|tpep_pickup_datetime_index|            features|        prediction|\n",
      "+--------+--------------------+---------------------+---------------+-------------+------------+------------+-----------+-----+----------+------------+-----------+---------------------------+--------------------------+--------------------+------------------+\n",
      "|       1|    02-01-2022 20:00|     02-01-2022 20:03|              1|          0.9|         236|          43|        5.0|  3.5|       0.0|         9.3|        0.0|                     4907.0|                    5681.0|[1.0,0.9,0.0,236....| 7.325684472461832|\n",
      "|       1|    02-01-2022 20:00|     02-01-2022 20:04|              0|          0.9|         230|         186|        5.0|  3.0|       2.0|        10.8|        0.0|                     3875.0|                    5681.0|[0.0,0.9,0.0,230....| 7.335673906152495|\n",
      "|       1|    02-01-2022 20:00|     02-01-2022 20:04|              2|          0.7|          43|         236|        4.5|  3.5|       0.0|         8.8|        0.0|                     3875.0|                    5681.0|[2.0,0.7,0.0,43.0...|  6.76598532718908|\n",
      "|       1|    02-01-2022 20:00|     02-01-2022 20:05|              1|          0.9|         162|         137|        6.0|  3.0|      1.95|       11.75|        0.0|                     6236.0|                    5681.0|[1.0,0.9,0.0,162....| 7.206579152383525|\n",
      "|       1|    02-01-2022 20:00|     02-01-2022 20:09|              1|          1.0|          90|         158|        7.0|  3.0|      2.15|       12.95|        0.0|                     4137.0|                    5681.0|[1.0,1.0,0.0,90.0...| 7.598296551711231|\n",
      "|       1|    02-01-2022 20:00|     02-01-2022 20:12|              2|          2.0|         249|         234|       10.0|  3.0|      2.75|       16.55|        0.0|                     4908.0|                    5681.0|[2.0,2.0,0.0,249....|10.207571469921898|\n",
      "|       1|    02-01-2022 20:01|     02-01-2022 20:04|              1|          0.6|         141|         263|        4.5|  2.5|      1.55|        9.35|        0.0|                     3875.0|                    7520.0|[1.0,0.6,0.0,141....| 6.453174761357875|\n",
      "|       1|    02-01-2022 20:01|     02-01-2022 20:09|              1|          1.0|         143|         163|        7.0|  3.5|      2.25|       13.55|        0.0|                     4137.0|                    7520.0|[1.0,1.0,0.0,143....| 7.559089773335944|\n",
      "|       1|    02-01-2022 20:01|     02-01-2022 20:11|              1|          1.4|         166|         238|        8.5|  3.0|      2.46|       14.76|        0.0|                     4672.0|                    7520.0|[1.0,1.4,0.0,166....| 8.573937289638781|\n",
      "|       1|    02-01-2022 20:01|     02-01-2022 20:13|              1|          2.1|         186|         125|       10.0|  3.0|      2.75|       16.55|        0.0|                     4909.0|                    7520.0|[1.0,2.1,0.0,186....|10.496288230277898|\n",
      "|       1|    02-01-2022 20:01|     02-01-2022 20:14|              1|          2.1|         107|         231|       10.0|  3.0|      3.45|       17.25|        0.0|                     5983.0|                    7520.0|[1.0,2.1,0.0,107....|10.387890106126495|\n",
      "|       1|    02-01-2022 20:01|     02-01-2022 20:15|              1|          2.7|         163|         238|       12.0|  3.0|       2.5|        18.3|        0.0|                     6479.0|                    7520.0|[1.0,2.7,0.0,163....| 11.97284474056937|\n",
      "|       1|    02-01-2022 20:01|     02-01-2022 20:22|              1|          4.0|         113|         239|       14.5|  3.0|      3.65|       21.95|        0.0|                     3878.0|                    7520.0|[1.0,4.0,0.0,113....| 15.64280133404752|\n",
      "|       1|    02-01-2022 20:02|     02-01-2022 20:11|              2|          1.7|         142|         239|        8.5|  3.0|      3.05|       15.35|        0.0|                     4672.0|                    4888.0|[2.0,1.7,0.0,142....| 9.428280250884686|\n",
      "|       1|    02-01-2022 20:02|     02-01-2022 20:12|              1|          3.4|          87|          79|       12.5|  3.0|       1.0|        17.3|        0.0|                     4908.0|                    4888.0|[1.0,3.4,0.0,87.0...|14.079053693154249|\n",
      "|       1|    02-01-2022 20:02|     02-01-2022 20:14|              0|          1.2|         234|          79|        8.5|  3.0|      2.45|       14.75|        0.0|                     5983.0|                    4888.0|[0.0,1.2,0.0,234....| 8.076741987549134|\n",
      "|       1|    02-01-2022 20:02|     02-01-2022 20:16|              1|          5.3|         262|          79|       17.0|  3.0|      4.15|       24.95|        0.0|                     3877.0|                    4888.0|[1.0,5.3,0.0,262....| 19.26696476069771|\n",
      "|       1|    02-01-2022 20:02|     02-01-2022 20:19|              1|          2.9|         113|          33|       12.5|  3.5|      3.35|       20.15|        0.0|                     7254.0|                    4888.0|[1.0,2.9,0.0,113....|12.604092981100594|\n",
      "|       1|    02-01-2022 20:03|     02-01-2022 20:06|              1|          0.5|          87|         231|        4.0|  3.0|       0.0|         7.8|        0.0|                     3876.0|                    4652.0|[1.0,0.5,0.0,87.0...| 6.253722174207616|\n",
      "|       1|    02-01-2022 20:03|     02-01-2022 20:17|              1|          3.5|         229|         202|       13.0|  3.0|       0.0|        16.8|        0.0|                     7942.0|                    4652.0|[1.0,3.5,0.0,229....|14.111956840251924|\n",
      "+--------+--------------------+---------------------+---------------+-------------+------------+------------+-----------+-----+----------+------------+-----------+---------------------------+--------------------------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = fit_model.transform(test_data)\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "090f20b8-ee80-4a14-9cd9-63ec188ec069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|fare_amount|        prediction|\n",
      "+-----------+------------------+\n",
      "|        5.0| 7.325684472461832|\n",
      "|        5.0| 7.335673906152495|\n",
      "|        4.5|  6.76598532718908|\n",
      "|        6.0| 7.206579152383525|\n",
      "|        7.0| 7.598296551711231|\n",
      "|       10.0|10.207571469921898|\n",
      "|        4.5| 6.453174761357875|\n",
      "|        7.0| 7.559089773335944|\n",
      "|        8.5| 8.573937289638781|\n",
      "|       10.0|10.496288230277898|\n",
      "|       10.0|10.387890106126495|\n",
      "|       12.0| 11.97284474056937|\n",
      "|       14.5| 15.64280133404752|\n",
      "|        8.5| 9.428280250884686|\n",
      "|       12.5|14.079053693154249|\n",
      "|        8.5| 8.076741987549134|\n",
      "|       17.0| 19.26696476069771|\n",
      "|       12.5|12.604092981100594|\n",
      "|        4.0| 6.253722174207616|\n",
      "|       13.0|14.111956840251924|\n",
      "+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.select(['fare_amount','prediction']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18991818-9b72-4028-929d-2be8ae96ca07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b42ebec-34ca-4a2f-9ccb-faf9bb75b1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test_results = fit_model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ca17849a-b081-4be0-aece-2405fedf6f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           residuals|\n",
      "+--------------------+\n",
      "|  -2.325684472461832|\n",
      "|  -2.335673906152495|\n",
      "|   -2.26598532718908|\n",
      "|  -1.206579152383525|\n",
      "| -0.5982965517112309|\n",
      "|-0.20757146992189845|\n",
      "| -1.9531747613578752|\n",
      "| -0.5590897733359439|\n",
      "|-0.07393728963878132|\n",
      "| -0.4962882302778979|\n",
      "|-0.38789010612649477|\n",
      "|0.027155259430630352|\n",
      "| -1.1428013340475207|\n",
      "| -0.9282802508846864|\n",
      "| -1.5790536931542487|\n",
      "|  0.4232580124508658|\n",
      "|  -2.266964760697711|\n",
      "|-0.10409298110059417|\n",
      "|  -2.253722174207616|\n",
      "| -1.1119568402519242|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results.residuals.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "725744d0-4aa8-4c2d-baf7-8fa73f2f04f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE:     5.617\n",
      "Ex Var:  98.906\n",
      "MAE:      2.085\n",
      "MSE:     31.549\n",
      "RMSE:     5.617\n",
      "R2:       0.754\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'RMSE:':7s} {test_results.rootMeanSquaredError:>7.3f}\")\n",
    "print(f\"{'Ex Var:':7s} {test_results.explainedVariance:>7.3f}\")\n",
    "print(f\"{'MAE:':7s} {test_results.meanAbsoluteError:>7.3f}\")\n",
    "print(f\"{'MSE:':7s} {test_results.meanSquaredError:>7.3f}\")\n",
    "print(f\"{'RMSE:':7s} {test_results.rootMeanSquaredError:>7.3f}\")\n",
    "print(f\"{'R2:':7s} {test_results.r2:>7.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d7d57d-ef27-47cc-a7bc-9542ec9f15f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0890541-2b97-41d9-8c69-250659feb7fc",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b943f2-b663-4603-a6ad-a141a27d86bb",
   "metadata": {},
   "source": [
    "Whether a taxi trip results in a tip or not. Here's a modified version of your code for logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cbbe45ee-aae5-4797-8f0e-54debc8b0b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dc4fb7ff-20ea-46dc-bb08-a3f4ec53da5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: string (nullable = true)\n",
      " |-- tpep_dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trip.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "97e53bfa-8531-4051-bbe5-59d8f13a1f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categorical and numeric columns\n",
    "categorical_columns = ['VendorID', 'PULocationID', 'DOLocationID']\n",
    "numeric_columns = ['passenger_count', 'trip_distance', 'extra', 'airport_fee']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b88dbc7f-486e-4c34-9a63-a47aa20bae9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexers = [StringIndexer(inputCol=col, outputCol=col + \"_index\", handleInvalid='keep') for col in categorical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e7e94d93-6855-4647-adce-d7b0492a5c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use StringIndexer to convert the categorical columns to hold numerical data\n",
    "VendorID_indexer=StringIndexer(inputCol='VenodorID', outputCol='VendorID_index',handleInvalid='keep')\n",
    "PULocationID_indexer=StringIndexer(inputCol='PULocationID', outputCol='PULocationID_index',handleInvalid='keep')\n",
    "DOLocationID_indexer=StringIndexer(inputCol='DOLocationID', outputCol='DOLocationID_index',handleInvalid='keep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2493a4a4-3ac0-48bb-82ae-1c959545ed12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3ef4735e-c771-4e6b-a230-25fecf173fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encoder = OneHotEncoder(\n",
    "    inputCols=[\n",
    "        'VendorID_index',\n",
    "        'PULocationID_index',\n",
    "        'DOLocationID_index'\n",
    "    ], \n",
    "    outputCols= [\n",
    "        'VendorID_vec',\n",
    "        'PULocationID_vec',\n",
    "        'DOLocationID_vec'],\n",
    "    handleInvalid='keep'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "78226192-e00d-4295-bea7-a925ae733254",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        'VendorID_vec',\n",
    "        'PULocationID_vec',\n",
    "        'DOLocationID_vec'\n",
    "        ],\n",
    "    outputCol=\"features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "49bbe838-0bc3-4971-ac48-9c5a6e52cf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=LogisticRegression(labelCol='tip_amount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6dd49bc7-450e-4d8e-8792-d6f7409a0c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(\n",
    "    stages=[\n",
    "        VendorID_indexer,\n",
    "    PULocationID_indexer,\n",
    "    DOLocationID_indexer,\n",
    "        data_encoder,\n",
    "        assembler,\n",
    "        lr_model\n",
    "    ]\n",
    ")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0fc333-c3cd-4b54-9dce-a1aedf207a11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "24d36941-4676-402a-8731-ac1048c2df5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o519.fit.\n: org.apache.spark.SparkException: Input column VenodorID does not exist.\n\tat org.apache.spark.ml.feature.StringIndexerBase.$anonfun$validateAndTransformSchema$2(StringIndexer.scala:128)\n\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\n\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\n\tat scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:198)\n\tat org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema(StringIndexer.scala:123)\n\tat org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema$(StringIndexer.scala:115)\n\tat org.apache.spark.ml.feature.StringIndexer.validateAndTransformSchema(StringIndexer.scala:145)\n\tat org.apache.spark.ml.feature.StringIndexer.transformSchema(StringIndexer.scala:252)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:237)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# run the pipeline\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m fit_model\u001b[38;5;241m=\u001b[39m\u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Store the results in a dataframe\u001b[39;00m\n\u001b[1;32m      5\u001b[0m results \u001b[38;5;241m=\u001b[39m fit_model\u001b[38;5;241m.\u001b[39mtransform(test_data)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pyspark/ml/pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    132\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mtransform(dataset)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m indexOfLastEstimator:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o519.fit.\n: org.apache.spark.SparkException: Input column VenodorID does not exist.\n\tat org.apache.spark.ml.feature.StringIndexerBase.$anonfun$validateAndTransformSchema$2(StringIndexer.scala:128)\n\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\n\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\n\tat scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:198)\n\tat org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema(StringIndexer.scala:123)\n\tat org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema$(StringIndexer.scala:115)\n\tat org.apache.spark.ml.feature.StringIndexer.validateAndTransformSchema(StringIndexer.scala:145)\n\tat org.apache.spark.ml.feature.StringIndexer.transformSchema(StringIndexer.scala:252)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:237)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "# run the pipeline\n",
    "fit_model=pipe.fit(train_data)\n",
    "\n",
    "# Store the results in a dataframe\n",
    "results = fit_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdede77f-62ee-4ee6-84a2-449a3715cc04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38dec30-fde2-416c-a699-74c13732d182",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "76a11bbf-45db-40be-bf15-5ac42c50d3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Vector Assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6ab312a1-d506-4b3b-aca3-377b18b31a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler=VectorAssembler(\n",
    "    inputCols=[\n",
    "        'passenger_count',\n",
    "        'trip_distance',\n",
    "        'airport_fee',\n",
    "        'extra',\n",
    "        'VendorID_index',\n",
    "        'PULocationID_index',\n",
    "        'DOLocationID_index'\n",
    "    ],\n",
    "    outputCol=\"features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6fe7c4fe-3cbc-4fb2-a1ed-efb87ae53f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline=Pipeline(stages=[\n",
    "    VendorID_indexer,\n",
    "    PULocationID_indexer,\n",
    "    DOLocationID_indexer,\n",
    "    assembler\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "870d1b04-1955-4509-96e3-3db5b4f5fcff",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o474.fit.\n: org.apache.spark.SparkException: Input column VenodorID does not exist.\n\tat org.apache.spark.ml.feature.StringIndexerBase.$anonfun$validateAndTransformSchema$2(StringIndexer.scala:128)\n\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\n\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\n\tat scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:198)\n\tat org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema(StringIndexer.scala:123)\n\tat org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema$(StringIndexer.scala:115)\n\tat org.apache.spark.ml.feature.StringIndexer.validateAndTransformSchema(StringIndexer.scala:145)\n\tat org.apache.spark.ml.feature.StringIndexer.transformSchema(StringIndexer.scala:252)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:237)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# run the pipeline\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m fit_model\u001b[38;5;241m=\u001b[39m\u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pyspark/ml/pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    132\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mtransform(dataset)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m indexOfLastEstimator:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o474.fit.\n: org.apache.spark.SparkException: Input column VenodorID does not exist.\n\tat org.apache.spark.ml.feature.StringIndexerBase.$anonfun$validateAndTransformSchema$2(StringIndexer.scala:128)\n\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\n\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\n\tat scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:198)\n\tat org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema(StringIndexer.scala:123)\n\tat org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema$(StringIndexer.scala:115)\n\tat org.apache.spark.ml.feature.StringIndexer.validateAndTransformSchema(StringIndexer.scala:145)\n\tat org.apache.spark.ml.feature.StringIndexer.transformSchema(StringIndexer.scala:252)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:237)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "# run the pipeline\n",
    "fit_model=pipeline.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6679f68a-ce48-4f0f-978f-51fd1c78c1dd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432f640c-59a8-46e5-84d8-d0abad611292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the training data\n",
    "preprocessing_model = pipeline.fit(train_data)\n",
    "train_data = preprocessing_model.transform(train_data)\n",
    "\n",
    "# Fit and transform the testing data\n",
    "test_data = preprocessing_model.transform(test_data)\n",
    "\n",
    "# Logistic Regression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Define the logistic regression model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"tip_amount\", maxIter=10)\n",
    "\n",
    "# Fit the model to the training data\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "predictions = lr_model.transform(test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
